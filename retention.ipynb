{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to refactor the code and parameterized the half-life predictor with pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variations of Half-life regression (HLR)\n",
    "\n",
    "short-hand for each record \\begin{align}<\\cdot>&=<\\Delta,x,P[\\text{recall}]\\in[0,1]>\\\\&=<\\Delta,x,y\\in\\{0,1\\}>\\end{align}\n",
    "\n",
    "Regression against recall probability $$l_\\text{recall}(<\\cdot>;\\theta)=(p-f_\\theta(x,\\Delta))^2$$\n",
    "\n",
    "Regression against back-solved half-life $$l_\\text{half-life}(<\\cdot>;\\theta)=(\\frac{-\\Delta}{\\log_2{p}}-f_\\theta(x,\\Delta))^2$$\n",
    "\n",
    "Binary recall classification $$l_\\text{binary}(<\\cdot>;\\theta)=\\text{xent}(f_\\theta(x,\\Delta),y)$$\n",
    "\n",
    "Assume that half-life increases exponentially with each repeated exposure, with a linear approximator, you get $f_\\theta(x,\\Delta)=2^{\\theta\\cdot x}$. Use this parameterization with regression against both recall probability and back-solved half-life, you get Settles' formulation:\n",
    "$$l(<\\cdot>; \\theta)=(p-2^{\\frac{\\Delta}{2^{\\theta\\cdot x}}})^2+\\alpha(\\frac{\\Delta}{\\log_2(p)}-2^{\\theta\\cdot{x}})^2+\\lambda|\\theta|_2^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import gzip\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_HALF_LIFE = 15.0 / (24 * 60)    # 15 minutes\n",
    "MAX_HALF_LIFE = 274.                # 9 months\n",
    "LN2 = math.log(2)\n",
    "\n",
    "def pclip(p):\n",
    "    # bound min/max model predictions (helps with loss optimization)\n",
    "    return min(max(p, 0.0001), .9999)\n",
    "\n",
    "def hclip(h):\n",
    "    # bound min/max half-life\n",
    "    return min(max(h, MIN_HALF_LIFE), MAX_HALF_LIFE)\n",
    "\n",
    "def featurize(df):\n",
    "    # disable feature by removing prefix f_\n",
    "    df.p_recall = df.p_recall.apply(pclip)\n",
    "    df.delta = df.delta.apply(lambda x: x / (60 * 60 * 24))  # in days\n",
    "    df['half_life'] = df.apply(lambda x: hclip(- x['delta'] / (math.log(x['p_recall'], 2))), axis=1)\n",
    "    # df['f_lang'] = df.apply(lambda x: '%s->%s' % (x['ui_language'], x['learning_language']), axis=1)\n",
    "    df['f_history_correct_sqrt'] = df.history_correct.apply(lambda x: math.sqrt(1 + x))\n",
    "    df['f_history_wrong_sqrt'] = df.apply(lambda x: math.sqrt(1 + x['history_seen'] - x['history_correct']), axis=1)\n",
    "    # df = df.rename({'delta': 'f_delta'}, axis=1)\n",
    "    df = df.rename({'history_seen': 'f_history_seen'}, axis=1)\n",
    "    df = df.rename({'history_correct': 'f_history_correct'}, axis=1)\n",
    "    # df = df.rename({'session_seen': 'f_session_seen'}, axis=1)\n",
    "    # df = df.rename({'session_correct': 'f_session_correct'}, axis=1)\n",
    "    df = df.rename({'history_correct_sqrt': 'f_history_correct_sqrt'}, axis=1)\n",
    "    df = df.rename({'history_wrong_sqrt': 'f_history_wrong_sqrt'}, axis=1)\n",
    "    df['bias'] = 1\n",
    "    return df\n",
    "\n",
    "def get_featurized_df():\n",
    "    featurized_df_dir = 'data/features.h5'\n",
    "\n",
    "    if os.path.exists(featurized_df_dir):\n",
    "        print('loading featurized_df')\n",
    "        return pd.read_hdf(featurized_df_dir)\n",
    "    \n",
    "    df = pd.read_csv('./data/settles.acl16.learning_traces.13m.csv.gz')\n",
    "    df = featurize(df)\n",
    "    \n",
    "    df.to_hdf(featurized_df_dir, 'data')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_split_dfs():\n",
    "    train_df_dir = 'data/train.h5'\n",
    "    test_df_dir = 'data/test.h5'\n",
    "\n",
    "    if os.path.exists(train_df_dir) and os.path.exists(test_df_dir):\n",
    "        print('loading train test df')\n",
    "        return pd.read_hdf(train_df_dir), pd.read_hdf(test_df_dir)\n",
    "\n",
    "    df = get_featurized_df()\n",
    "    \n",
    "    splitpoint = int(0.9 * len(df))\n",
    "    train_df, test_df = df.iloc[:splitpoint], df.iloc[splitpoint:]\n",
    "    \n",
    "    train_df.to_hdf(train_df_dir, 'data')\n",
    "    test_df.to_hdf(test_df_dir, 'data')\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "def get_split_numpy():\n",
    "    dirs = [\n",
    "        'data/x_train.npy',\n",
    "        'data/y_train.npy',\n",
    "        'data/x_test.npy',\n",
    "        'data/y_test.npy'\n",
    "    ]\n",
    "    if all(os.path.exists(d) for d in dirs):\n",
    "        print('loading train test numpy')\n",
    "        return (np.load(d) for d in dirs)\n",
    "    \n",
    "    train_df, test_df = get_split_dfs()\n",
    "    \n",
    "    feature_names = [c for c in train_df.columns if c.startswith('f_')] + ['bias']\n",
    "    print('features', feature_names)\n",
    "    x_train = train_df[feature_names].to_numpy().astype(np.float32)\n",
    "    y_train = train_df['p_recall'].to_numpy().astype(np.float32)\n",
    "    x_test = test_df[feature_names].to_numpy().astype(np.float32)\n",
    "    y_test = test_df['p_recall'].to_numpy().astype(np.float32)\n",
    "    \n",
    "    np.save(dirs[0], x_train)\n",
    "    np.save(dirs[1], y_train)\n",
    "    np.save(dirs[2], x_test)\n",
    "    np.save(dirs[3], y_test)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetentionDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, fold='train'):\n",
    "        x_train, y_train, x_test, y_test = get_split_numpy()\n",
    "        self.mean = np.mean(x_train, axis=0)\n",
    "        self.std = np.std(x_train, axis=0)\n",
    "        self.mean[-1] = 0\n",
    "        self.std[-1] = 1\n",
    "\n",
    "        if fold == 'train':\n",
    "            self.x, self.y = x_train, y_train\n",
    "        elif fold == 'test':\n",
    "            self.x, self.y = x_test, y_test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        x = (self.x[idx] - self.mean) / self.std\n",
    "        y = np.array(self.y[idx])\n",
    "        return torch.from_numpy(x), torch.from_numpy(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features ['f_history_seen', 'f_history_correct', 'f_history_correct_sqrt', 'f_history_wrong_sqrt', 'bias']\n"
     ]
    }
   ],
   "source": [
    "train_dataset = RetentionDataset('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-27908426b9a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# self.fc1 = nn.Linear(n_input, 128)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, n_input):\n",
    "        super(Net, self).__init__()\n",
    "        # self.fc1 = nn.Linear(n_input, 128)\n",
    "        # self.dropout1 = nn.Dropout(0.25)\n",
    "        # self.fc2 = nn.Linear(128, 2)\n",
    "        self.fc1 = nn.Linear(n_input, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.fc1(x)\n",
    "        # x = F.relu(x)\n",
    "        # x = self.dropout1(x)\n",
    "        # x = self.fc2(x)\n",
    "        # return x\n",
    "        return self.fc1(x)\n",
    "\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(data)\n",
    "        loss = loss_func(logits, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test(args, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    loss_func = nn.CrossEntropyLoss(reduction='sum')\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            logits = model(data)\n",
    "            # sum up batch loss\n",
    "            test_loss += loss_func(logits, target).item()\n",
    "            # get the index of the max log-probability\n",
    "            pred = logits.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            predictions += pred[:, 0].detach().cpu().numpy().tolist()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return test_loss, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Retention model')\n",
    "    parser.add_argument('--batch-size', type=int, default=1024, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "    parser.add_argument('--epochs', type=int, default=6, metavar='N',\n",
    "                        help='number of epochs to train (default: 14)')\n",
    "    parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n",
    "                        help='learning rate (default: 0.001)')\n",
    "    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
    "                        help='Learning rate step gamma (default: 0.7)')\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=1000, metavar='N',\n",
    "                        help='how many batches to wait before logging status')\n",
    "    args = parser.parse_args()\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "    train_dataset = RetentionDataset('train')\n",
    "    test_dataset = RetentionDataset('test')\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=args.test_batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "    n_input = train_dataset.x.shape[1]\n",
    "    model = Net(n_input=n_input).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "    best_test_loss = 9999\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(args, model, device, train_loader, optimizer, epoch)\n",
    "        test_loss, predictions = test(args, model, device, test_loader)\n",
    "        scheduler.step()\n",
    "        if test_loss < best_test_loss:\n",
    "            checkpoint_dir = \"checkpoints/retention_model.pt\"\n",
    "            torch.save(model.state_dict(), checkpoint_dir)\n",
    "            print('save model checkpoint to', checkpoint_dir)\n",
    "            best_test_loss = test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
